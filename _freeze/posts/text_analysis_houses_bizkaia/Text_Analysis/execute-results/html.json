{
  "hash": "e69bc8e2a8867f181c8aa15ec2f0e8d2",
  "result": {
    "markdown": "---\ntitle: \"Text_Analysis\"\nsubtitle: \"An Example using Spanish House Prices from Kaggle\"\nauthor: \"Txomin Basterra Chang\"\ndate: \"2023-02-25\"\nimage: \"image.png\"\ncategories: [housing, Spain, code, analysis]\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\nlibrary(data.table)\nlibrary(grDevices)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(tidyverse)\nlibrary(tidytext)\n```\n:::\n\n\nNowadays, economic data often contains information in text form. The Spanish Houses dataset from Kagglecontains sales information on Spanish real estate of March 2017. In this exercise, I want to give a brief demonstration of text analysis in R.\n\nThe data is coming from the Kaggle data set on [Spanish houses](https://www.kaggle.com/datasets/thedevastator/spanish-housing-dataset-location-size-price-and). I am using the here-package to preserve the folder structure and easily find the files of interest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(here(\"Data\", \"Houses_Bizkaia\", \"data.Rda\"))\n```\n:::\n\n\nBefore we start, some cleaning: We are only interested in houses which are located in Bizkaia. Also, we want to restrict the data to houses which are for sale but not for rent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data %>% filter(is.na(ad_description))) [1] / dim(data) [1]  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03276286\n```\n:::\n\n```{.r .cell-code}\ndata = data %>% \n  drop_na(ad_description) %>% \n  filter(loc_zone != \"Álava\") %>%\n  filter(loc_city != \"Vitoria-Gasteiz\") %>%\n  filter(!grepl(\"Alquiler\", house_type)) %>% # Zur Miete\n  filter(room_num != \"sin habitación\") %>%\n  mutate(room_num = as.numeric(room_num))\n```\n:::\n\n\nThe text data can be found under the variable `ad_description`. Using it we can build our `corpus` for analysis. To analyse the occurence frequence of word, each word needs to be assigned a row. This can be done using the function `unnest_tokens()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus = tibble(line = 1:dim(data) [1], text = data$ad_description)  %>%\n  unnest_tokens(word, text) %>% na.omit()\n```\n:::\n\n\nNaturally, The corpus contains alot of stop words. Stop words like \"and\", \"it\", or \"this\" are are essential, they however do not contain context specific information. Because our corpus is in Spanish, we use the dictionary `tm::stopwords(\"spanish\")` from the `tm` text mining package to load in the beeded stop words. We use `anti_join()` to then delete them from the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspanish_stop_words <- bind_rows(stop_words,\n                               tibble(word = tm::stopwords(\"spanish\"),\n                                          lexicon = \"custom\")) %>% filter(lexicon == \"custom\")\n\ncorpus = corpus %>% anti_join(spanish_stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n:::\n\n\nLets have a look at the results: Because there are alot of word frequencies alanysed, we will only plot words which are occuring more than 5000 times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_freq = corpus %>%\n  count(word, sort = TRUE)\n\nglimpse(corpus_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 20,789\nColumns: 2\n$ word <chr> \"cocina\", \"baño\", \"salón\", \"piso\", \"vivienda\", \"habitaciones\", \"z…\n$ n    <int> 19732, 18120, 15118, 14923, 14175, 14076, 12512, 11975, 10843, 10…\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_freq %>%\n  filter(n > 5000) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n```\n\n::: {.cell-output-display}\n![](Text_Analysis_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Text_Analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}