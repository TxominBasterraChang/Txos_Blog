[
  {
    "objectID": "posts/Bike_Munich/Bike_Munich.html",
    "href": "posts/Bike_Munich/Bike_Munich.html",
    "title": "Fahrradaktivität in München",
    "section": "",
    "text": "Auf dem Open Data Portal der Stadt München finden sich viele interessante Datensätze zu allen Möglichen Themen. Auf diesem befinden sich unteranderem die Daten der Raddauerzählstellen, welche eine herausragende Möglichkeiten bieten einen Einblick in die Fahrradaktivität in der Stadt München zu erlangen.\nDie Observationen werden an verschiedenen, innerhalb der City verteilten Zählstationen getätigt. Dabei wird im 15 Minuten Tackt gezählt wie viele Fahrradfahrerinnen innerhalb dieser Zeitspanne an der jeweilige\nBei dem Datensatz, den ich in dieser Analyse vorstellen werde handelt es sich um Tageswerte. Dabei werden die 15 minütigen Messungen auf den Tag kumuliert. Dieser Datensatz beinhalten neben der Fahrrad-Tagesaktivität auch Wetterdaten, wie etwa die Temperatur oder Sonnenstunden. Die Zeitreihe beginnt im Juni 2008 und läuft bis Dezember 2022."
  },
  {
    "objectID": "posts/Bike_Munich/Bike_Munich.html#fahrradaktivität-nach-zählstationen",
    "href": "posts/Bike_Munich/Bike_Munich.html#fahrradaktivität-nach-zählstationen",
    "title": "Fahrradaktivität in München",
    "section": "Fahrradaktivität nach Zählstationen",
    "text": "Fahrradaktivität nach Zählstationen\nMünchen hat 6 Zählstation, welche über die Stadt verteilt sind. Wir sehen in der unteren Karte, dass die Stationen alle mehr oder weniger zentral gelegen sind. Nicht alle Stationen haben im selben Jahr angefangen zu messen. Mehr Informationen lassen sich durch das Klicken auf die Punkte herausfinden.\n\nload_github_data(\"https://github.com/TxominBasterraChang/txos_blog/blob/main/Data/Bike_Munich/data_zaehl.Rda\")\n\n mapview(data_zaehl, zcol = \"zaehlstelle\") \n\n\n\n\n\n\nDie untere Grafik zeigt auf wie sich die Aktivität der verschiedenen Stationen über die Jahre entwickelt hat. Wie bei der Gesamtaktivität lässt sich auch bei den meisten Zählstationen ein zyklisches Wachstum beobachten. Auffällig sind die Frequenzunterschiede zwischen den Stationen. In manchen ist scheint die Aktivität durchgehend höher zu sein als in anderen.\n\ndata %>%\n  group_by(datum, zaehlstelle) %>%\n  summarise(Gesamt = sum(gesamt), \n            Min.temp = mean(min.temp),\n            saison = saison) %>%\n  distinct() %>%\n  ggplot(aes(x = datum, y = Gesamt, color = zaehlstelle)) +\n  facet_wrap(~ zaehlstelle, ncol = 3) +\n  geom_line() +\n  ggtitle(\"Jahresentwicklung\") +\n  theme_minimal() +\n  theme(axis.title.x=element_blank())\n\n\n\n\nEbenfalls interessant sind Ausreißer wie etwa bei der Zählstation Olympia (zwischen den Jahren 2012 und 2015). Diese sind vermutlich durch das 24h Mountain Bike Rennen im Olympia Park erzeugt worden (Eine Analyse dazu findet sich in SOMTOMS Blog).\nDas Gesamtaktivitätsprofil lässt sich auch gut mit Boxplots darstellen. Die horizontal schraffierte Linie stellt den Gesamtmedian dar. Erhardt und Margareten sind die Stationen mit der höchsten Aktivität, deren Aktivität deutlich über dem Populationsmedian liegt.\n\nMedian <- data %>%\n  summarise(Median = median(gesamt))\nMedian <- Median$Median\n\ndata %>%\n  group_by(zaehlstelle) %>%\n  ggplot(aes(x = zaehlstelle, y = gesamt, fill = zaehlstelle)) +\n  geom_boxplot() +\n  geom_hline(yintercept = Median, linetype=\"dashed\", color = \"black\") + \n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  ggtitle(\"Boxplots Tagesaktivität per Zählstelle\")"
  },
  {
    "objectID": "posts/Bike_Munich/Bike_Munich.html#wochentage",
    "href": "posts/Bike_Munich/Bike_Munich.html#wochentage",
    "title": "Fahrradaktivität in München",
    "section": "Wochentage",
    "text": "Wochentage\n\ndata %>%\n  group_by(datum) %>%\n  summarise(Gesamt = sum(gesamt), \n            wday = wday) %>%\n  mutate(wday = case_when(wday == 1 ~ \"Son\",\n                             wday == 2 ~ \"Mo\",\n                             wday == 3 ~ \"Di\",\n                             wday == 4 ~ \"Mi\",\n                             wday == 5 ~ \"Do\",\n                             wday == 6 ~ \"Fr\",\n                             wday == 7 ~ \"Sa\")) %>%\n  distinct() %>%\n  ggplot(aes(x = wday, y = Gesamt, fill = wday)) +\n  geom_boxplot() +\n  geom_hline(yintercept = 7693, linetype=\"dashed\", color = \"black\") + \n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  ggtitle(\"Boxplots Tagesaktivität per Wochentag\") \n\n\n\n\nAm Wochenende wird weniger Aktivität gemessen. Die Wochenend-mediane liegen unterhalb des Gesamtmedian. Mehr Informationen über die tägliche und Wöchentliche Fahrradaktivität finden sich in dieser Shiny App."
  },
  {
    "objectID": "posts/Bike_Munich/Bike_Munich.html#wetter-und-aktivität",
    "href": "posts/Bike_Munich/Bike_Munich.html#wetter-und-aktivität",
    "title": "Fahrradaktivität in München",
    "section": "Wetter und Aktivität",
    "text": "Wetter und Aktivität\nSchauen wir uns die Beziehung zwischen dem Wetter und der Fahrradaktivität an.\n\np1 = data %>%\n  group_by(datum) %>%\n  summarise(Gesamt = sum(gesamt), \n            Min.temp = mean(min.temp),\n            saison = saison) %>%\n  distinct() %>%\n  ggplot(aes(x = Min.temp, y = Gesamt)) +\n  geom_point(aes(color = saison), size=0.7) +\n  geom_smooth() +\n  theme_minimal() \n\n\np2 = data %>%\n  group_by(datum) %>%\n  summarise(Gesamt = sum(gesamt), \n            Max.temp = mean(max.temp),\n            saison = saison) %>%\n  distinct() %>%\n  ggplot(aes(x = Max.temp, y = Gesamt)) +\n  geom_point(aes(color = saison), size=0.7, show.legend = FALSE) +\n  geom_smooth() +\n  theme_minimal() \n\np3 = data %>%\n  group_by(datum) %>%\n  summarise(Gesamt = sum(gesamt), \n            Niederschlag = mean(niederschlag),\n            saison = saison) %>%\n  distinct() %>%\n  ggplot(aes(x = Niederschlag, y = Gesamt)) +\n  geom_point(aes(color = saison), size=0.7, show.legend = FALSE) +\n  geom_smooth() +\n  theme_minimal() \n\np4 = data %>%\n  group_by(datum) %>%\n  summarise(Gesamt = sum(gesamt), \n            Bewoelkung = mean(bewoelkung),\n            saison = saison) %>%\n  distinct() %>%\n  ggplot(aes(x = Bewoelkung, y = Gesamt)) +\n  geom_point(aes(color = saison), size=0.7, show.legend = FALSE) +\n  geom_smooth() +\n  theme_minimal() \n\np5 = data %>%\n  group_by(datum) %>%\n  summarise(Gesamt = sum(gesamt), \n            Sonnenstunden = mean(sonnenstunden),\n            saison = saison) %>%\n  distinct() %>%\n  ggplot(aes(x = Sonnenstunden, y = Gesamt)) +\n  geom_point(aes(color = saison), size=0.7, show.legend = FALSE) +\n  geom_smooth() +\n  theme_minimal() \n\n\nggarrange(p1, p2, p4, p5, p3,\n          ncol = 2, nrow = 3, common.legend = TRUE)\n\n\n\n\nMin und Max Temperatur sind positiv mit der Aktivität assoziiert. Das Verhältnis scheint dabei linear zu sein. D.h. dass eine Temperaturerhöhung um n Grad ungefähr eine Aktivitätssteigerung um x Fahrradfahrerinnen mit sich bringt. Ähnliches gilt für Sonnenstunden. Bei der Bewölkung haben wir eine negative Assoziation.\nAuf den ersten Blick scheint die Stärke des Niederschlags keinen besonderen Einfluss auf Fahrradaktivität zu haben. Das liegt vielleicht daran, dass es in München generell wenig regnet und der Trend durch große Regen-Ausreißer verzerrt wird. Beschränken wir die Observation auf das 95% Quantil der Regen Daten.\n\nquantile(data$niederschlag, probs = 0.95)\n\n\ndata %>%\n  group_by(datum) %>%\n  filter(niederschlag <= 12.1 ) %>%\n  summarise(Gesamt = sum(gesamt), \n            Niederschlag = mean(niederschlag)) %>%\n  distinct() %>%\n  ggplot(aes(x = Niederschlag, y = Gesamt)) +\n  geom_point(shape = 15, size = 2) +\n  geom_smooth() +\n  theme_minimal() +\n  labs(title = \"Niederschlag und Aktivität\",\n       subtitle = \"Beschränkt auf das 95% Quantil\")\n\n\n\n\nBei kleinen Werten scheint es einen leichten negativen Trend zu geben. Allgemein betrachtet ist der Trend jedoch schwachen."
  },
  {
    "objectID": "posts/Bike_Munich/Bike_Munich.html#regression",
    "href": "posts/Bike_Munich/Bike_Munich.html#regression",
    "title": "Fahrradaktivität in München",
    "section": "Regression",
    "text": "Regression\nEin einfaches Model für das Verständnis der Einflussfaktoren ist die lineare Regression:\n\nmodel <- lm(gesamt ~ min.temp + niederschlag + bewoelkung + sonnenstunden + wday, data = data )\n\n\n\nplot_summs(model)\n\n\n\n\nWir sehen, dass die Sonnenstunden den größten Einfluss auf die Tagesaktivität hat. Noch größer als die Temperatur. Je sonniger es in München ist, desto mehr wird an den Stationen gemessen.\nInteressanterweise scheint Niederschlag nun einen signifikanten negativen Einfluss auf die Aktivität zu haben. Das dieses Verhältnis vorher nicht aufgefallen ist mag vielleicht daran gelegen haben, dass Niederschlag positiv mit dem positiven Einflussfaktor Temperatur korreliert ist und der negative Effekt dadurch übertüncht wurde."
  },
  {
    "objectID": "posts/Historical_Background/HB.html",
    "href": "posts/Historical_Background/HB.html",
    "title": "Politics and the Press in Switzerland",
    "section": "",
    "text": "The Swiss Federal System\nFederalism and direct democracy are major features of the Swiss political system. Every year multiple ballots or “Abstimmung” are held in each canton where citizens are required to vote on important communal decisions such as governmental spendings, investment in infrastructure, schooling or defense issues. In this work I will not focus on this aspect of the Swiss direct democracy but on the Swiss federal elections. Elections or “Wahlen” differ from ballots in the sense that they are periodically repeating events with a homogenous character compared to very heterogenous aspects in public-affairs-decision making (Poledna, 2021). Within the period of investigation the Swiss federal election for the Swiss parliament were held roughly every three to four years. The electoral system is based on the Swiss federal constitution of 1848 which grants every adult male citizens aged 20 + the right to vote. Cantons are given freedom regarding the exact implementation of the voting law. This includes the enforcement of sanctioned compulsory voting rules which were introduced by 13 cantons for particular election years during the period of investigation (Hangartner et al., 2011). The first election took place in 1848 and was then held every 3 years until 1931 with 1919 being an exception. In 1931 the term of office was extended to four years.\nThe year 1918 marked one of the most important turning points in Swiss electoral history. After being one of the main topics of public political discourse over the years and after two failed referenda, the electoral system switched from being held in a majoritarian fashion where the “winner takes it all” to a proportional system where parliamentary seats were allocated according to the percentage of votes (Petrov, 2018). As a result, smaller parties that were highly disadvantaged under the previous majoritarian system, gained governmental influence after the reform. Until then, Switzerland’s political system was mostly dominated by liberal-radical parties (Gruner et al., 1990). The event of 1918 was accompanied by social uprising and political conflict. In a phase of economic instability stemming from the repercussions of the First World War, the urge for political change was rising. After liberal-radicals again won the parliamentary elections in 1917 despite of the salient drop in vote share, discontent about political stagnation lead to a series of strikes and finally culminated in a general strike which started in Zürich and quickly spread into the whole of Switzerland. Under the pressure of the general strike the election planned for 1920 was advanced to 1919.\nAfterwards, liberal-radical groups lost their dominance which was then followed by the rise of social- democratic, conservative and the so called Bauern-, Gewerbe- und Bürger parties (Petrov, 2018). Figure 8 depicts the aggregated vote share of the liberal parties FL (Freisinnige Linke) and LM (Liberale Mitte), the gains and losses the liberals registered between elections (D_liberals), as well as the aggregated vote share of all other elected parties in the federal elections (1848-1955). In 1893 the FL changed its name to FDP (Freisinnig Demokratische Partei der Schweiz) and in 1914 the LM changed its name to LPS (Liberale Partei der Schweiz).\nAmong liberal parties, the FL/and later FDP always received the highest vote share. Up to 1928 the FL/FDP constantly achieved the highest vote share among all parties. After that, it was surpassed by the rise of the social democrats SP (Sozialdemokratische Partei der Schweiz).\nStriking is the sharp drop of liberal vote share in the election of 1917 one years before the introduction of the proportional system and the following gradual diminution.\n\n\n\n\nTURNOUT_31 <- read_excel(here(\"Data\", \"Historical_Background\", \"Voteshare_liberals.xlsx\"))\nA <- TURNOUT_31\nA = A %>% mutate(liberals = FL + LM)\nA = A %>% mutate(others = 100 - liberals)\nA = A %>% mutate(delta_liberals = diff_Fl + diff_LM)\nA = A %>% select(-c(FL, LM, diff_Fl, diff_LM))\nA_long<- gather(A, party, outcomme, liberals:delta_liberals, factor_key=TRUE)\n\nggplot(data = A_long, aes(x = Year, y = outcomme, group = party, colour = party)) +\n  geom_line() +\n  theme_ipsum() + \n  ggtitle(\"Figure 1: Turnout (1848-1955)\") +\n  labs(x = \"Year\", y =  \"Turnout in %\") \n\n\n\n\n\n\nDevelopment of Voter Turnout\nLooking at the entire voting history of federal elections, Switzerland experienced an W-shape development in electoral participation. In the first elections starting 1848 roughly 44.6% of eligible voters went to vote. During the period of political dominance by the liberal radicals before 1919, turnout averaged 54.8%. Between 1919 and 1935, after the introduction of the proportional voting system, electoral participation reached an average level of 80%. In the period after the extension of the term in office to 4 years, participation decreased to 60%. Starting 1979, which lies outside of my investigation, turnout dropped for the first time below 50% (Neidhart, 2017).\n\ni_am(\"index.qmd\")\nTURNOUT_31 <- read_excel(here(\"Data\", \"Historical_Background\", \"TURNOUT.xlsx\"))\nA <- TURNOUT_31\n\nggplot(data = A, aes(x = Year, y = Turnout, group = Year)) +\n  geom_boxplot() +  \n  stat_summary(fun.y=mean, aes(group=language, colour = language), geom = \"line\", lwd=0.5, lty=1) +\ntheme_ipsum() + \n  ggtitle(\"Figure 2:Turnout (1848-1955)\") +\n  labs(x = \"Year\", y =  \"Turnout in %\") \n\n\n\n\nFigure 2 depicts boxplots per election term for turnout within the period 1848-1955. Striking is the transition between 1917 and 1919 where electoral participation increased significantly throughout all language regions. Also, after 1919 there seems to be an increased number of outlier cantons which experienced significantly lower turnout results than the majority. Noticeable is also that voter turnout in French-speaking regions is almost always lower compared to the German ones. Italian speaking regions lie mostly in between.\n\n\nExpansion of Newspapers\nThe development of the press went along the rise of the bourgeoise in the 19th century. Previously, a phase of slow growth in newspapers preceded until the end of the 18th century. The years after the breakdown of the “13örtige Eidgenossenschaft” under the Helvetic republic with the proclamation of press freedom in 1798, the start of the “Regeneration”- period in 1831, and the establishment of the national congress in 1848 (embedding freedom of the press into the constitution) all experienced a large increase in newspaper founding (Blaser, 1954; Clavien et al. 2015).\nSwitzerland’s liberal press regulations facilitated the formation of many opinion presses which, due to the country’s small scale, were often addressing a local audience on regional politics. During the period of regeneration opinion presses were an important factor through which press freedom was promoted. The commercialization of the press started differently for different language regions: First in the French then in the German and not until the end of the 20th century in the Italian speaking regions.\nIn German language regions the professionalization at the end of the 19th century was followed by the foundation of the association of Swiss press (Verein der Schweizer Presse) and the foundation of the Swiss publishers association (Schweizerischer Verlegerverband). Technical improvements like the introduction of rotary printing in 1890 and the linotype machine in 1893 greatly improved productivity and reduced the costs of printing. The emergence of news agencies facilitated the collection of information for the associated newspapers; and the development of infrastructure for distribution and postal services enabled large scale circulation. Investment in new technologies mainly required funding through advertisement (Genzkow et al. 2014, Clavien et al 2014), which was followed by the emergence of profit oriented non-partisan newspapers. On the demand site, increasing literacy rates starting from 1850 can also account for the expansion of newspapers (Messerli and Mathieu, 1992, p. 177-178). In German speaking regions the appearance of literature clubs (Lesegesellschaften) provided an inexpensive access to newspapers which reasonably extended readership.\n\nload(here(\"Data\", \"Historical_Background\", \"N_combined12.Rda\"))\nA <- N_combined12\nnames(A)[names(A) == \"Kanton_ID\"] <- \"canton\"\n\n\nggplot(data = A %>% subset(Year >= 1848)  %>% subset(Year <= 1955), \n       aes(x = Year, y = numb_news, group = Year)) +\n  geom_boxplot() +\n  stat_summary(fun.y=mean, aes(group=canton, colour = canton), \n               geom = \"line\", lwd=0.5, lty=1, \n                 data = . %>% filter(canton %in% c(\"Zürich\" , \"Bern\", \"Vaud\"))) +\n  theme_ipsum() + \n  ggtitle(\"Newspapers (1848-1955)\") +\n  labs(x = \"Year\", y =  \"Newspapers\") \n\n\n\n\nThe graphics depict different aspects of Swiss newspaper growth. Figure 10 clearly shows that the net increase of newspapers is positive over the years (presented are yearly boxplots of newspaper numbers per canton). Notable is that the cantons Bern, Vaud and Zürich exhibit significantly larger newspaper numbers compared to the rest of Switzerland."
  },
  {
    "objectID": "posts/Crime_Chicago/Dashboard.html",
    "href": "posts/Crime_Chicago/Dashboard.html",
    "title": "Crimes in Chicago",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(timetk)\nlibrary(kableExtra)\nlibrary(highcharter)\nlibrary(sf)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(plotly)\n\nload(here(\"Data\", \"Crime_Chicago\", \"Chicago\", \"data.Rda\"))\nDATA = data\n\n\nAnalyse\n\nAllgemein\nZum Datensatz: Die Daten zu den Kriminalstatistiken stammen von Kaggle. Das Shapfile von Chicago stammt von der Homepage der City of Chicago. Folgende Varaiablen finden sich in der Kriminalstatistik:\n\nnames(DATA)\n\n [1] \"ID\"                   \"Case Number\"          \"Date\"                \n [4] \"Block\"                \"IUCR\"                 \"Primary Type\"        \n [7] \"Description\"          \"Location Description\" \"Arrest\"              \n[10] \"Domestic\"             \"Beat\"                 \"District\"            \n[13] \"Ward\"                 \"Community Area\"       \"FBI Code\"            \n[16] \"X Coordinate\"         \"Y Coordinate\"         \"Year\"                \n[19] \"Updated On\"           \"Latitude\"             \"Longitude\"           \n[22] \"Location\"            \n\n\nDie Deliktinformationen lassen sich wie folgt kategorisieren:\n\ninhaltlich: “Primary Type”, “Description”, “Arest”)\nzeitlich: “Date”, “Updated On”\nörtlich: “Location”, “Domestic”, “Beat”, “District”, “Ward”, “Community Area”, “X Coordinate”, “Y Coordinate”, “Latitude”, “Longitude”\nformale: “ID”, “Case Number”, FBI Code”\n\nDie allgemeinste inhaltliche Kategorisierung erfolgt durch den “Primary Type”. Die Variable “Description” unterteilt jeden “Primary Type” jeweils noch in Subkategorien.\nSpezielles Augenmerk wird in diesem Dashboard auf den “Primary Type” “Narcotics” gelegt, welcher auf Platz 4 der häufigsten Delikttypen steht.\n\n\nGesamtdelikte\nBevor Drogendelikte betrachtet werden, schauen wir uns den gesamten Datensatz gebündelt an, um einen Überblick über die Gesamtentwicklung zu bekommen.\n\nDATA %>%   \n  mutate(Date = date(Date)) %>%\n  tabyl(Date) %>%\n  mutate(Year = substr(Date, 1, 4)) %>%\n  ggplot(aes(x = Date, y = n, group = Year)) + \n  geom_boxplot() +\n  ggtitle(\"Gesamtdelikte (Boxplots pro Jahr)\") +\n  theme_minimal() \n\n\n\n\nDelikte nehmen über die Zeit ab. Im Dashboard kann man ebenfalls sehen, dass die Fallzahlen über den Jahresverlauf Zyklen Weise auf- und abschwellen. Auffällig ist, dass am 31. Dezember jeden Jahres keine Delikte gemeldet sind. Besonders sticht die Fallzahl des 31. Mai 2020 heraus, welche wahrscheinlich mit den Ausschreitungen um den Tod von George Floyd zusammenhängt.\n\n\nNarcotics\nWie oben beschrieben stehen etwa 7.5% aller Delikte im Zusammenhang mit Drogen. Innerhalb der Drogendelikte stellen Cannabis und Heroinbesitz die zwei häufigsten registrierten Straftaten dar. Cannabis (<30g) in 39.5% und Heroin (Weiß) in 14.5% der Fälle.\nAuffällig ist der stätige Rückgang der Cannabisdelikte, welche besonders stark zwischen den Jahren 2015-2017 vonstatten gegangen ist. Bis in die Gegenwart hat sich das Level auf einem niedrigen Niveau eingependelt.\nDelikte bzgl. Heroinbesitzt haben sich über die Jahre nicht stark verändert und lagen bis Dezember 2016 immer unterhalb denen von Cannabis. Ähnlich wie bei Cannabis sind die Werte innerhalb der Jahren 2015-2017 (wenn auch nur schwach) gefallen.\nDer Besitz von Cannabis wurde im Staate Illinois (Chicago) zum 01.01.2020 legalisiert. Dennoch finden sich in der Statistik immer noch Fälle, die über dieses Datum hinaus gehen.\n\n\nWieso sind die Zahlen gefallen?\nIm Jahre 2015 verkündete das Polizeipräsidium von Chicago ein neues Vorgehen, in welchem Bußgelder anstatt von Strafeinträgen in Fällen von Cannabisbesitz erteilt werden sollten, welches die Festnahmen deutlich senkte. Das Vorgehen folgt einem Bürgerentscheid aus dem Jahre 2012 nach, infolgedessen Cannabisbesitz in kleinen Mengen nicht strafrechtlich, sondern zivilrechtlich behandelt werden sollten.\nIm Jahre 2017 wurde im Staate Illinois ein Gesetz verabschiedet, welches den Konsum von medizinischen Marihuana erlaubt. Dies hat wohl auch zur Verringerung der Fallzahlen beigetragen.\n\n\nRäumliche Verteilung\nDrogendelikte und Kriminelle Aktivitäten im Allgemeinen konzentrieren sich stark auf den Bezirk Austin und die anliegenden Bezirke Humboldt Park, North Lawndale und West Garfield Park.\n\nAustin als Problembezirk\nAustin ist einer der 77 Bezirke Chicago und befindet sich im Westen der Stadt. Er ist der dritt größte Bezirk nach Bevölkerung und der zweit größte nach Fläche.\nAustin ist immer wieder wegen krimineller Aktivitäten negativ in die Schlagzeilen gekommen.\n\nhttps://abc7chicago.com/chicago-crime-man-stabbed-austin-police/12715681/\nhttps://www.chicagotribune.com/news/ct-crime-in-chicago-20171114-storygallery.html\n\nDie Daten des Dashboards zeigen insbesondere, dass Drogendelikte stark in diesem Bezirk konzentriert sind. Die Verteilung der Drogendelikte spiegelt die generelle Verteilung der Drogendelikte in ganz Chicago wider.\n\n\n\n\nDashboard\n\nGesammtdelikte\n\np = data %>%   \n  mutate(Date = date(Date)) %>%\n  tabyl(Date) %>%\n  dplyr::rename(Anzahl = n) %>%\n  ggplot(aes(x = Date, y = Anzahl)) + \n  geom_point(size = 0.2) +\n  geom_smooth() + \n  theme_minimal()\n\nggplotly(p)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nCanabis- und Heroin Delikte\n\nx1 <- data %>% \n  mutate(Date = date(Date)) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  filter(Description == \"POSS: CANNABIS 30GMS OR LESS\") %>%\n  tabyl(Date) %>%\n  mutate(Group = \"CANNABIS\")\n\nx2 <- data %>% \n  mutate(Date = date(Date)) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  filter(Description == \"POSS: HEROIN(WHITE)\") %>%\n  tabyl(Date) %>%\n  mutate(Group = \"HEROIN\")\n\nx3 <- data %>% \n  mutate(Date = date(Date)) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  filter(Description == \"POSS: HEROIN(WHITE)\") %>%\n  tabyl(Date) %>%\n  mutate(Group = \"HEROIN\")\n\nx = rbind(x1, x2)\nx = x %>% dplyr::rename(Anzahl = n)\n\np = x %>%\n  ggplot(aes(x = Date, y = Anzahl, colour = Group)) + \n  geom_point(size = 0.2) +\n  geom_smooth() + \n  theme_minimal()\n  \nggplotly(p)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nDelikt Kategorien\n\ndata %>% \n  tabyl(`Primary Type`) %>% \n  filter(percent > 0.01) %>%\n  mutate(percent = percent*100) %>%\n  arrange(desc(n)) %>%\n  hchart(\"bar\",\n         hcaes(x = `Primary Type`, y = percent)) %>%\n  hc_add_theme(hc_theme_smpl())\n\n\n\n\n\n\n\n\nArten von Drogen Delikten\n\ndata %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  tabyl(Description) %>% \n  filter(percent > 0.01) %>%\n  mutate(percent = percent*100) %>%\n  arrange(desc(n)) %>%\n  hchart(\"bar\",\n         hcaes(x = Description, y = percent)) %>%\n  hc_add_theme(hc_theme_smpl())\n\n\n\n\n\n\n\n\n\nKarte\n\nKummulierte Gesamtdelikte nach Community Areas\n\nCount_total = DATA %>%\n  tabyl(`Community Area`) %>%\n  arrange(desc(n)) %>%\n  mutate(percent = percent*100) %>%\n  dplyr::rename(Comm_Area = `Community Area`) %>%\n  dplyr::rename(percent_total = percent) %>%\n  dplyr::rename(n_total = n) \n\nCount_narco = DATA %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  tabyl(`Community Area`) %>%\n  arrange(desc(n)) %>%\n  mutate(percent = percent*100) %>%\n  dplyr::rename(Comm_Area = `Community Area`) %>%\n  dplyr::rename(percent_narco = percent) %>%\n  dplyr::rename(n_narco = n) \n\n\nmap = read_sf(here(\"Data\", \"Crime_Chicago\",\"Boundaries_Community_Areas\", \"geo_export_ef0a0074-492a-41aa-90f3-965e6165ec88.shp\"))\nmap = map %>% dplyr::rename(Comm_Area = area_num_1) %>%\n  select(-c(area, area_numbe, comarea, comarea_id, perimeter))\n\nmap = merge(x = map, y = Count_total, by = c(\"Comm_Area\"), all.x = TRUE)\nmap = merge(x = map, y = Count_narco, by = c(\"Comm_Area\"), all.x = TRUE)\n\nggplot() + \n  geom_sf(data = map, mapping = aes(fill = n_total)) +\n  theme_minimal()\n\n\n\n\n\n\nKummulierte Drogendelikte nach Community Areas\n\nggplot() + \n  geom_sf(data = map, mapping = aes(fill = n_narco)) +\n  theme_minimal()\n\n\n\n\n\n\nGesamtdelikte (> 2% der Gesamtdelikte)\n\nmap %>% \n  filter(percent_total > 2) %>%\n  mutate(percent_total = round(percent_total, 4)) %>%\n  arrange(desc(percent_total)) %>%\n  hchart(\"bar\",\n         hcaes(x = community, y = percent_total)) %>%\n  hc_add_theme(hc_theme_smpl())\n\n\n\n\n\n\n\n\nDrogendelikte (> 2% der Drogendelikte)\n\nmap %>% \n  filter(percent_narco > 2) %>%\n  mutate(percent_narco = round(percent_narco, 4)) %>%\n  arrange(desc(percent_narco)) %>%\n  hchart(\"bar\",\n         hcaes(x = community, y = percent_narco)) %>%\n  hc_add_theme(hc_theme_smpl())\n\n\n\n\n\n\n\n\n\nAustin\n\nDrogen in Austin\n\nAustin = DATA %>%\n  filter(`Community Area` ==25) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  tabyl(`Description`) %>%\n  arrange(desc(n)) %>%\n  dplyr::rename(percent_narco = percent) %>%\n  dplyr::rename(n_narco = n) \n\n\nx1 =  DATA %>%\n  filter(`Community Area` ==25) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  filter(`Description` == \"POSS: CANNABIS 30GMS OR LESS\") %>%\n  mutate(Date = date(Date)) %>%\n  tabyl(Date) %>%\n  mutate(Group = \"CANNABIS\")\n\nx2 =  DATA %>%\n  filter(`Community Area` ==25) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  filter(`Description` == \"POSS: HEROIN(WHITE)\") %>%\n  mutate(Date = date(Date)) %>%\n  tabyl(Date) %>%\n  mutate(Group = \"HEROIN\")\n\nx3 =  DATA %>%\n  filter(`Community Area` ==25) %>%\n  filter(`Primary Type` == \"NARCOTICS\") %>%\n  filter(`Description` == \"POSS: CRACK\") %>%\n  mutate(Date = date(Date)) %>%\n  tabyl(Date) %>%\n  mutate(Group = \"CRACK\")\n\nx = rbind(x1, x2, x3)\nx = x %>% dplyr::rename(Anzahl = n)\n\np = x %>%\n  ggplot(aes(x = Date, y = Anzahl, fill = Group)) + \n  geom_bar(position=\"stack\", stat=\"identity\") +\n  geom_smooth() +\n  theme_minimal()\nggplotly(p)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nArten von Drogen Delikten\n\nAustin %>%\n  filter(percent_narco > 0.01) %>%\n  mutate(percent_narco = percent_narco*100) %>%\n  arrange(desc(percent_narco)) %>%\n  hchart(\"bar\",\n         hcaes(x = Description, y = percent_narco)) %>%\n  hc_add_theme(hc_theme_smpl())\n\n\n\n\n\n\n\n\n\nMonatsentwicklung\n\nDurchschnittliche Anzahl an Delikten nach Monaten\n\ndata %>%\n  mutate(Date = date(Date)) %>%\n  tabyl(Date) %>%\n  mutate(Month = month(Date)) %>%\n  group_by(Month) %>%\n  summarise(Month_mean = mean(n)) %>%\n  hchart(\"column\",\n         hcaes(x = Month, y = Month_mean))"
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html",
    "title": "Houses in Bilbao",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\nlibrary(openxlsx)\nlibrary(lubridate)\nlibrary(here)\nlibrary(devtools)\nlibrary(recipes) \nlibrary(rsample)\nlibrary(timetk)\nlibrary(glmnet)\nlibrary(tidyquant)\nlibrary(visdat)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(caret)\nlibrary(rpart)  \nlibrary(rpart.plot)\nlibrary(pdp)\nlibrary(vip)\nlibrary(GGally)\nlibrary(car)\nlibrary(ggcorrplot)\nlibrary(ggdensity)\nlibrary(tidyquant)\nlibrary(scico)\nlibrary(paletteer)\nlibrary(earth)\nlibrary(vip)\nlibrary(ranger)\nlibrary(h2o)\nlibrary(xgboost)\nlibrary(modeltime)\nlibrary(caret)\nlibrary(lares)\nlibrary(lmtest)\nlibrary(nortest)\nlibrary(auditor)\nlibrary(DALEXtra)\nlibrary(modelStudio)\nlibrary(patchwork)\nWhat determines the price of houses in Bilbao? I want to investigate this question using Kaggle’s Spanish Housing Dataset which was originally web-crawled from Idealista (between March-April 2019).\nKnowledge of the housing market is not only valuable for sellers and buyers of real estate but can also provide profound understanding of socio-economic and socio-demographic local variations within a city.\nBilbao is a city in northern Spain located in the province Bizkaia. It is the largest metropolitan area in the region and an economic and cultural hub.\nIn this analysis I will explore Bilbao’s housing market in search for an understanding of the driving factors of real estate prices."
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#splitting-the-data",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#splitting-the-data",
    "title": "Houses in Bilbao",
    "section": "Splitting the Data",
    "text": "Splitting the Data\nFirst we split the data into a train and test set where 3/4 of the whole data is reserved for training and the rest for testing. We also use strata = \"price_m2\" to make sure that the distribution of price_m2 is equal between training and testing set.\n\nset.seed(123)\nsplit <- initial_split(Bilbao, prop = 0.75, \n                       strata = \"price_m2\")\ndata_train  <- training(split)\ndata_test   <- testing(split)"
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#missing-data",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#missing-data",
    "title": "Houses in Bilbao",
    "section": "Missing Data",
    "text": "Missing Data\nThere is missing data. Because some models (like OLS) cannot handle NAs we will interpolate these using a KNN Algorithm.\n\nvis_miss(Bilbao, cluster = TRUE)"
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#create-a-recipe",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#create-a-recipe",
    "title": "Houses in Bilbao",
    "section": "Create a Recipe",
    "text": "Create a Recipe\nThe function recipe() allows to preprocess the data before modeling. It can be applied to training and testing data. The advantage of uf using recipe() is that it avoids data leakage between data sets. Data leakage occurs when data transformation steps are processed on the entire data set before it is subdivided into training and testing splits. E.g., if a min-max transformation would be applied on the whole data set before splitting, the individual splits would be biased towards to global minimum and maximum. In a resampling and cross validation context, recipe() ensures that the data preprocessing is conducted after every iteration of data splitting.\n\nmodel_rec <- recipe(\n  price_m2 ~ .,\n  data= data_train) %>% \n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal()) %>%\n  step_impute_knn(all_predictors(), neighbors = 10) %>% \n  prep(training = data_train, retain=TRUE, verbose=TRUE)\n\noper 1 step zv [training] \noper 2 step dummy [training] \noper 3 step impute knn [training] \nThe retained training set is ~ 0.97 Mb  in memory.\n\ntrainSet.prep <- bake(model_rec, new_data = data_train, composition='matrix')\ntrainSet = as.data.frame((trainSet.prep))\n\ntestSet.prep<-bake(model_rec, new_data = data_test, composition='matrix')\ntestSet = as.data.frame((testSet.prep))"
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#initialize-models",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#initialize-models",
    "title": "Houses in Bilbao",
    "section": "Initialize Models",
    "text": "Initialize Models\n\nmyControl = trainControl(method = 'cv', \n                         number = 10, \n                         repeats = 5,\n                         verboseIter = FALSE, \n                         savePredictions = TRUE,\n                         allowParallel = T)\n\n\nparallel_start(6)\n\nset.seed(174)\nLinear.Model = train(price_m2 ~., \n                     data = trainSet, \n                     metric = 'RMSE', \n                     method = 'lm',\n                     preProcess = c('center', 'scale'),\n                     trControl = myControl)\n\n\nset.seed(174)\nGlmnet.Model = train(price_m2 ~ ., \n                     data = trainSet , \n                     metric = 'RMSE', \n                     method = 'glmnet',\n                     preProcess = c('center', 'scale'), \n                     trControl = myControl)\n\n\nset.seed(174)\nRapid.Ranger = train(price_m2 ~ ., \n                     data = trainSet, \n                     metric = 'RMSE', \n                     method = 'ranger',\n                     preProcess = c('center', 'scale'),\n                     trControl = myControl)\n\n\nset.seed(174)\nBasic.Knn <- train(price_m2 ~ .,\n             method     = \"knn\",\n             tuneGrid   = expand.grid(k =1:3), \n             trControl  = myControl, \n             metric= \"RMSE\", \n             data = trainSet)\n\n\nset.seed(174)\nXgb.Super <- train(price_m2~.,\n                   method = \"xgbTree\", \n                   tuneLength = 4,\n                   trControl = myControl,\n                   metric= \"RMSE\", \n                   data = trainSet)\n\nparallel_stop()\n\n\nsuite.of.models = list(\"LINEAR.MODEL\" = Linear.Model,\n                       \"GLMNET.MODEL\" = Glmnet.Model, \n                       \"RANGER.QUEST\" = Rapid.Ranger, \n                       \"KNN.SIMPLE\" = Basic.Knn, \n                       \"XGB.SUPER\"= Xgb.Super)\n\nresamps = resamples(suite.of.models) \ndotplot(resamps, metric = 'RMSE')\n\n\n\n\nXGBoost perfomes best on the training data. Let’s test it on the test data."
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#testing-models-on-test-set",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#testing-models-on-test-set",
    "title": "Houses in Bilbao",
    "section": "TESTING MODELS ON TEST SET",
    "text": "TESTING MODELS ON TEST SET\n\nEvaluate.Prediction <- function(model, model.label, testData, ytest, grid = NULL) {\n \n  #capture prediction time\n  ptm <- proc.time()\n  # use test data to make predictions\n  pred <- predict(model, testData)\n  tm <- proc.time() - ptm\n  \n  Pred.metric<- postResample(pred = pred, obs = ytest)\n  RMSE.test <- c(Pred.metric[[1]])\n  RSquared.test <- c(Pred.metric[[2]])\n  MAE.test <- c(Pred.metric[[3]])\n  \n  \n  Summarised.results = NULL\n  if (is.null(grid)) { \n    Summarised.results = data.frame(predictor = c(model.label) ,  RMSE = RMSE.test , RSquared = RSquared.test, MAE = MAE.test, time = c(tm[[3]]))\n  } else {\n    .grid = data.frame(predictor = c(model.label) , RMSE = RMSE.test , RSquared = RSquared.test, MAE = MAE.test, time = c(tm[[3]]))\n    Summarised.results = rbind(grid, .grid)}\n  \n  \n  Summarised.results }\n\n\nMETRIC.GRID <- Evaluate.Prediction (Rapid.Ranger, \"RAPID.QUEST\", testSet, testSet$price_m2, grid=NULL)\n\nMETRIC.GRID <- Evaluate.Prediction (Glmnet.Model, \"GLMNET.MODEL\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Basic.Knn, \"KNN.SIMPLE\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Linear.Model, \"LINEAR.MODEL\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Xgb.Super, \"XGB.SUPER\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\n\nkable(METRIC.GRID[order(METRIC.GRID$RMSE, decreasing=F),]) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n      \n    predictor \n    RMSE \n    RSquared \n    MAE \n    time \n  \n \n\n  \n    5 \n    XGB.SUPER \n    773.6237 \n    0.6066140 \n    576.2671 \n    0.035 \n  \n  \n    1 \n    RAPID.QUEST \n    805.4272 \n    0.5733575 \n    584.3447 \n    0.032 \n  \n  \n    2 \n    GLMNET.MODEL \n    822.0240 \n    0.5559522 \n    607.2007 \n    0.013 \n  \n  \n    4 \n    LINEAR.MODEL \n    822.3261 \n    0.5553493 \n    607.3631 \n    0.006 \n  \n  \n    3 \n    KNN.SIMPLE \n    1023.4154 \n    0.3434201 \n    720.6059 \n    0.063 \n  \n\n\n\n\n\nAlso here, XGBoost has the smallest RMSE and highest R2. Let’s tune this model for further examination."
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#preprocessing",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#preprocessing",
    "title": "Houses in Bilbao",
    "section": "Preprocessing",
    "text": "Preprocessing\nFirst we start again by dividing the data into training and testing samples. Again, we use price_m2 as a strata to ensure that the distribution of the response is eqaul between the testing and training sets. Then we create a recipe.\n\nset.seed(123)\nsplit <- initial_split(Bilbao, prop = 0.75, \n                       strata = \"price_m2\")\ndata_train  <- training(split)\ndata_test   <- testing(split)\n\nXGB_rec <- recipe(\n  price_m2 ~ .,\n  data= data_train) %>% \n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal()) %>%\n  step_impute_knn(all_predictors(), neighbors = 10) %>% \n  prep()\n\nApply pre-processing to randomly divide train data in subsets.\n\nset.seed(123)\ncv_folds <-recipes::bake(\n    XGB_rec, \n    new_data = data_train)%>%  \n  rsample::vfold_cv(v = 5)\n\ntrain.ready<-juice(XGB_rec)\ntest.ready<-bake(XGB_rec, new_data = data_test)"
  },
  {
    "objectID": "posts/Houses_Bilbao/Houses_Bilbao.html#modelling-specifications",
    "href": "posts/Houses_Bilbao/Houses_Bilbao.html#modelling-specifications",
    "title": "Houses in Bilbao",
    "section": "Modelling specifications",
    "text": "Modelling specifications\nDefine XGBoost modelling specifications and hyper parameters for tuning.\n\nModel.XGB <- \n  boost_tree(\n    mode = \"regression\",\n    trees = 1000,\n    min_n = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %>% \n  set_engine(\"xgboost\", objective = \"reg:squarederror\")\n\n\nSpecify the model parameters\n\n# grid specification \nXGB.aspects <- \n  dials::parameters(\n    min_n(),\n    tree_depth(),\n    learn_rate(),\n    loss_reduction())\n\n\n\nGrid Space\nSet up a grid space which covers the hyper parameters XGB.aspects.\n\nxgboost_grid <- \ndials::grid_max_entropy(\nXGB.aspects, size = 200)\nkable(head(xgboost_grid))\n\n\n\n \n  \n    min_n \n    tree_depth \n    learn_rate \n    loss_reduction \n  \n \n\n  \n    19 \n    4 \n    0.0000000 \n    0.0024421 \n  \n  \n    15 \n    11 \n    0.0502801 \n    0.0000204 \n  \n  \n    38 \n    3 \n    0.0000000 \n    0.0000000 \n  \n  \n    7 \n    10 \n    0.0000001 \n    0.0002759 \n  \n  \n    38 \n    8 \n    0.0000000 \n    0.0000001 \n  \n  \n    37 \n    5 \n    0.0287556 \n    0.0000003 \n  \n\n\n\n\n\n\n\nCreate a workflow\n\nxgboost_wf <- \nworkflows::workflow() %>%\nadd_model(Model.XGB) %>% \nadd_formula(price_m2 ~ .)\n\n\n\nHyper parameter searching\nIn this step R searches for the optimal hyper parameters by iteratively applying the different hyper parameters to multiple training samples. This step can take a while to compute.\n\n'parallel_start(6)\n\nTUNE.XGB <- tune::tune_grid(\n  object = xgboost_wf,\n  resamples = cv_folds,\n  grid = xgboost_grid,\n  metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::rsq_trad, yardstick::mae),\n  control = tune::control_grid(verbose = FALSE)) \n\nparallel_stop()\n\nsaveRDS(TUNE.XGB, file = here(\"Data\", \"Tune_XGB.RData\"))'\n\n\n\nFinalize optimal tune\nExtract parameters with lowest RMSE.\n\nTUNE.XGB <- readRDS(here(\"Data\", \"Houses_Bilbao\",\"Tune_XGB.RData\"))\n\nparam_final <- TUNE.XGB %>%select_best(metric = \"rmse\")\n\nFinalize XGBoost with optimal tune.\n\nxgboost_wf2 <- xgboost_wf%>%\nfinalize_workflow(param_final)\n\n\n\nFit the final model\nFit final model on the preprocessed training data.\n\nXGB.model <- xgboost_wf2 %>%\nfit(train.ready)\n\nExtract important features.\n\nXGB.model %>% \n  pull_workflow_fit() %>% \n  vip()\n\n\n\n\nThe number of bathrooms, whether the appartment has a lift and the construction date seems to be the most important determinants for the square meter price.\n\n\nPredict on Test set.\nLet’s evaluate the XGBoost model on the test data by computing the RMSE.\n\n# use the training model fit to predict the test data\nXGB_res <- predict(XGB.model, new_data = test.ready %>% select(-price_m2))\n\nXGB_res <- bind_cols(XGB_res, test.ready %>% select(price_m2))\n\nXGB_metrics <- metric_set(yardstick::rmse, yardstick:: mae)\n\nkable(XGB_metrics(XGB_res, truth = price_m2, estimate = .pred))\n\n\n\n \n  \n    .metric \n    .estimator \n    .estimate \n  \n \n\n  \n    rmse \n    standard \n    799.5876 \n  \n  \n    mae \n    standard \n    590.3934 \n  \n\n\n\n\n\nWe can asses the fit of the pridiction by plotting them against the actual observations from the testing samples. We can see that the model works good up to 5000 Euro per square meter. For higher the model tends to underestimate the real prices.\n\nggplot(XGB_res, aes(x = price_m2, y = .pred)) + \n    # Create a diagonal line:\n    geom_abline(lty = 2) + \n    geom_point(alpha = 0.5) + \n    labs(y = \"Predicted Sale Price\", x = \"SalePrice\") +\n    # Scale and size the x- and y-axis uniformly:\n    coord_obs_pred() +\n  theme_minimal()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Txo’s Blog",
    "section": "",
    "text": "Fahrradaktivität in München\n\n\nEine Analyse der Raddauerzählstellen 2008-2022\n\n\n\n\nMunich\n\n\nBike\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nTxomin Basterra Chang\n\n\n\n\n\n\n  \n\n\n\n\nHouses in Bilbao\n\n\nA little Investigation\n\n\n\n\nhousing\n\n\nSpain\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\nTxomin Basterra Chang\n\n\n\n\n\n\n  \n\n\n\n\nCrimes in Chicago\n\n\nA little Investigation\n\n\n\n\nCrime\n\n\nChicago\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nTxomin Basterra Chang\n\n\n\n\n\n\n  \n\n\n\n\nPolitics and the Press in Switzerland\n\n\nA little Historical investigation with R\n\n\n\n\nhistory\n\n\nSwitzerland\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nTxomin Basterra Chang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]