{"title":"Houses in Bilbao","markdown":{"yaml":{"title":"Houses in Bilbao","subtitle":"A little Investigation","author":"Txomin Basterra Chang","date":"2023-04-15","image":"image.png","categories":["housing","Spain","code","analysis"],"editor":"visual"},"headingText":"Selecting the Data","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n```\n\n```{r, message=FALSE}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\nlibrary(openxlsx)\nlibrary(lubridate)\nlibrary(here)\nlibrary(devtools)\nlibrary(recipes) \nlibrary(rsample)\nlibrary(timetk)\nlibrary(glmnet)\nlibrary(tidyquant)\nlibrary(visdat)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(caret)\nlibrary(rpart)  \nlibrary(rpart.plot)\nlibrary(pdp)\nlibrary(vip)\nlibrary(GGally)\nlibrary(car)\nlibrary(ggcorrplot)\nlibrary(ggdensity)\nlibrary(tidyquant)\nlibrary(scico)\nlibrary(paletteer)\nlibrary(earth)\nlibrary(vip)\nlibrary(ranger)\nlibrary(h2o)\nlibrary(xgboost)\nlibrary(modeltime)\nlibrary(caret)\nlibrary(lares)\nlibrary(lmtest)\nlibrary(nortest)\nlibrary(auditor)\nlibrary(DALEXtra)\nlibrary(modelStudio)\nlibrary(patchwork)\n```\n\nWhat determines the price of houses in Bilbao? I want to investigate this question using Kaggle's [Spanish Housing Dataset](https://www.kaggle.com/datasets/thedevastator/spanish-housing-dataset-location-size-price-and) which was originally web-crawled from [Idealista](https://www.idealista.com) (between March-April 2019).\n\nKnowledge of the housing market is not only valuable for sellers and buyers of real estate but can also provide profound understanding of socio-economic and socio-demographic local variations within a city.\n\nBilbao is a city in northern Spain located in the province Bizkaia. It is the largest metropolitan area in the region and an economic and cultural hub.\n\nIn this analysis I will explore Bilbao's housing market in search for an understanding of the driving factors of real estate prices.\n\n\nFirst, let's have a look at the features and the presence of missing data.\n\n```{r}\nload(file = here(\"Data\", \"Houses_Bilbao\" ,\"data.Rda\"))\n\nvis_miss(data, cluster = TRUE)\n```\n\nThe data has many variables, but we are particularly interested in features such as **construction year**, **district**, **number of rooms**, **lift** and **number of bathrooms**, etc. to regress on our response **price per square meter**.\n\nLet's first drop features which have no data to build our working data set.\n\n```{r}\nBilbao <- data %>%\n  filter(loc_city == \"Bilbao\") %>%\n  select(-c(loc_city, garage, ad_description, ad_last_update, ground_size, kitchen, unfurnished, house_per_city, house_id, floor, loc_zone, loc_street, loc_neigh, obtention_date, orientation, loc_full, m2_useful)) %>%\n  mutate(price_m2 = price/m2_real) %>%\n  mutate(loc_district = gsub(\"Distrito\", \"\", loc_district)) %>%\n  mutate(bath_num = as.numeric(bath_num)) %>%\n  filter(price_m2 >= quantile(price_m2, probs = 0.001)) %>%\n  filter(price_m2 <= quantile(price_m2, probs = 0.999)) %>%\n  mutate(condition = case_when(condition == \"segunda mano/buen estado\" ~ \"used_ok\",\n                               condition == \"promoción de obra nueva\" ~ \"new\",\n                               condition == \"segunda mano/para reformar\" ~ \"used_to_reno\")) %>%\n  mutate(heating_type = case_when(heating == \"calefacción central\" ~ \"central\",\n                             heating == \"calefacción central: gas\" ~ \"central\",\n                             heating == \"calefacción central: gas propano/butano\" ~ \"central\",\n                             heating == \"calefacción central: gasoil\" ~ \"central\",\n                             heating == \"calefacción individual\" ~ \"individual\",\n                             heating == \"calefacción individual: bomba de frío/calor\" ~ \"individual\",\n                             heating == \"calefacción individual: eléctrica\" ~ \"individual\",\n                             heating == \"calefacción individual: gas\" ~ \"individual\",\n                             heating == \"calefacción individual: gas natural\" ~ \"individual\",\n                             heating == \"calefacción individual: gas propano/butano\" ~ \"individual\",\n                             heating == \"calefacción individual: gas propano/butano\" ~ \"individual\",\n                             heating == \"no dispone de calefacción\" ~ \"no_heat\"\n                             )) %>%\n  mutate(id = row_number()) %>%\n  filter(id != 3239) %>%\n  select(-c(id, m2_real, heating, price)) \n\n# Save File\nsave(Bilbao, file = here(\"Data\", \"Houses_Bilbao\", \"Bilbao.Rda\"))\n```\n\n# Summary Statistics\n\n```{r}\nBilbao %>%\n  tabyl(loc_district) %>%\n  ggplot(aes(x = \"\", y = percent, fill = loc_district)) +\n           geom_bar(stat = \"identity\", width =1, color=\"white\") +\n           coord_polar(\"y\" , start = 0) +\n  scale_fill_tq() +\n  theme_minimal() +\n  ggtitle(\"Proportion of houses per district\")\n```\n\nThe graphic shows the proportion houses per district. In some cases two adjacent districts were grouped together in the data.\n\n# Square Meter Prices per District\n\nRemoving outliers \\<01 and \\>99 quantiles\n\n```{r}\nBilbao %>%\n  group_by(loc_district) %>%\n  summarise(Min = min(price_m2),\n            Q25 = quantile(price_m2, probs = .25),\n            Median = median(price_m2),\n            Mean = mean(price_m2),\n            Q75 = quantile(price_m2, probs = .75),\n            Max = max(price_m2)) %>%\n  arrange(desc(Median)) %>%\n  dplyr::rename(District = loc_district) %>%\n  kable(digits = 0, caption = \"Statistic of price_m2 per district\") %>%\n  kable_styling(full_width = T)\n```\n\nIt is better to visualize the table in a boxplot.\n\n```{r}\nBilbao %>%\n  summarise(Min = min(price_m2),\n            Q25 = quantile(price_m2, probs = .25),\n            Median = median(price_m2),\n            Mean = mean(price_m2),\n            Q75 = quantile(price_m2, probs = .75),\n            Max = max(price_m2)) %>%\n  arrange(desc(Median))\n```\n\n```{r}\nBilbao %>%\n  group_by(loc_district) %>%\n  ggplot(aes(x = loc_district, y = price_m2, fill = loc_district)) +\n  geom_boxplot() +\n  geom_hline(yintercept = 2962.963, linetype=\"dashed\", color = \"black\") + # general Median\n  scale_fill_tq() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  ggtitle(\"Boxplots of price_m2 per district\") +\n  scale_y_continuous(labels = scales::dollar_format(prefix = \"€\"))\n```\nThe districts Abando/Albia and Indautxu stand out for having particularly high square meter prices which are pushing the population median upwards. The differences between the other districts are comparabily not so high.  \n\nLet's also have a look on the general distribution of `price_m2`\n\n```{r}\nBilbao %>%\n  ggplot(aes(x = price_m2)) +\n  geom_histogram(bins = 100) +\n  ggtitle(\"House price/m2 in Euro\") +\n  scale_x_continuous(labels = scales::dollar_format(prefix = \"€\")) +\n  theme_minimal() \n```\n\nThe distribution is relatively normal with a slight skewness to the right. \n\n# Overview of house features\n\n```{r, fig.width=11, fig.height=8}\nBilbao %>%\n  filter(room_num <= 5) %>%\n  tabyl(room_num, loc_district) %>%\n  pivot_longer(\n    cols = c(2:13),\n    names_to = \"loc_district\",\n    values_to = \"Count\"\n  ) %>%\n  group_by(loc_district) %>%\n  mutate(Sum = sum(Count)) %>%\n  mutate(Perc = Count/Sum * 100) %>%\n  ggplot(aes(x = loc_district, y = Perc, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  facet_grid(cols = vars(room_num)) +\n  scale_fill_tq() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  labs(title = \"Percentage of Rooms per District\")\n```\nMost dwellings have 3 rooms. Interestingly Casco Viejo has a ove rproportionally many one room appartments while Indautxu and Abando have proportionally the largest number of 5 rooms.\n\n```{r, fig.width=11, fig.height=8}\nBilbao %>%\n  ggplot(aes(x = heating_type, y = price_m2, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  scale_fill_tq() +\n  theme_minimal() +\n  labs(title = \"Price_m2 per Heating Type\") +\n  scale_y_continuous(labels = scales::dollar_format(prefix = \"€\"))\n```\nThe type of heating doesn't seem to affect very much. \n\n```{r, fig.width=11, fig.height=8}\nBilbao %>%\n  tabyl(condition, loc_district) %>%\n  pivot_longer(\n    cols = c(2:13),\n    names_to = \"loc_district\",\n    values_to = \"Count\"\n  ) %>%\n  group_by(loc_district) %>%\n  ggplot(aes(x = loc_district, y = Count, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  facet_grid(cols = vars(condition)) +\n  scale_fill_tq() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  labs(title = \"Absolute Count of House Condition per District\")\n```\nThe majority of houses in the markets are second hand appartments in good condition.\n\n```{r, fig.width=11, fig.height=8}\nBilbao %>%\n  ggplot(aes(x = condition, y = price_m2, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  scale_fill_tq() +\n  theme_minimal() +\n  labs(title = \"Price_m2 per Condition\") \n```\nAt first sight, it seems counter intuitive that new apartments cost less than second hand per sqare meter. Because of the hilliness of Bilbao, it might be that the new buildings are build in more peripheral areas of a district and therefore have a lower square meter price.\n\n# Construction Date\n\n```{r}\nscatterplot(price_m2 ~ construct_date, data = Bilbao, grid = FALSE)\n```\nWe see that there obvious relation between construction year and square meter price. Ad the same time we can observe that a great amount of houses were built during the period 1960-1975 corresponding to large migration from other Spanish provinces. \n\n# Development of Housing and Price\n\n```{r, fig.width=11, fig.height=8}\n# Hiogh density region\nBilbao %>%\n  ggplot(aes(x =construct_date, y = price_m2, fill = loc_district)) +\n  geom_hdr(probs = c (0.9, 0.5)) +\n  geom_point(shape = 21, size = 1.5) +\n  scale_fill_tq() +\n  theme_tq() +\n  labs(title = \"High density regions (Price_m2 ~ Date)\") +\n  facet_wrap(~ loc_district)\n```\nWe can see that houses within district often share similar price ranges and construction years. Some older district live through a longer period of continuous construction, whereas in other construction seemed to had ben concentrated in smaller period. \n\n```{r, fig.width=11, fig.height=8}\nBilbao %>%\n  filter(loc_district == \" Abando - Albia\" | loc_district == \" San Ignacio\" | loc_district == \" Otxarkoaga - Txurdinaga\") %>%\n  ggplot(aes(x =construct_date, y = price_m2, fill = loc_district)) +\n  geom_hdr(probs = c (0.9, 0.5)) +\n  geom_point(shape = 21, size = 1.5) +\n  scale_fill_tq() +\n  theme_tq() +\n  labs(title = \"High density regions certain Neighborhoods\")\n```\nThe clustering is specially visible for theese districts in a single graph.\n\n# Looking at Correlations\n\nFirst, let us look at the correlations between all numeric features and response. \n\n```{r, message=FALSE, fig.width=7, fig.height=7}\n\nnumeric = Bilbao %>% \n  select(where(is.numeric)) \n\nggcorr(numeric, label = TRUE, label_size = 2, label_color = \"white\") +\n  scale_fill_paletteer_c(\"viridis::plasma\") +\n  labs(title = \"Correlations (Numeric Variables)\")\n```\n\nEspecially important are the correlations with response variable. \n\n```{r, message=FALSE, fig.width=7, fig.height=7}\nBilbao %>%\n  corr_var(price_m2)\n```\nUsing the simple correlation we can see that being loacted in the district of Abando/Albia has the highest positive correlation with the square meter price. \n\n# Modeling\nNow, since we got an overview of the data, we can start to analyzenthe relationships between the features and the response more in depth.\n\n## Splitting the Data\n\nFirst we split the data into a train and test set where 3/4 of the whole data is reserved for training and the rest for testing. We also use `strata = \"price_m2\"` to make sure that the distribution of price_m2 is equal between training and testing set.\n\n```{r}\nset.seed(123)\nsplit <- initial_split(Bilbao, prop = 0.75, \n                       strata = \"price_m2\")\ndata_train  <- training(split)\ndata_test   <- testing(split)\n```\n\n## Missing Data\n\nThere is missing data. Because some models (like OLS) cannot handle NAs we will interpolate these using a KNN Algorithm.\n\n```{r}\nvis_miss(Bilbao, cluster = TRUE)\n```\n\n## Create a Recipe\n\nThe function `recipe()` allows to preprocess the data before modeling. It can be applied to training and testing data. The advantage of uf using `recipe()` is that it avoids data leakage between data sets. Data leakage occurs when data transformation steps are processed on the entire data set before it is subdivided into training and testing splits. E.g., if a min-max transformation would be applied on the whole data set before splitting, the individual splits would be biased towards to global minimum and maximum. In a resampling and cross validation context, `recipe()` ensures that the data preprocessing is conducted after every iteration of data splitting.\n\n```{r}\nmodel_rec <- recipe(\n  price_m2 ~ .,\n  data= data_train) %>% \n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal()) %>%\n  step_impute_knn(all_predictors(), neighbors = 10) %>% \n  prep(training = data_train, retain=TRUE, verbose=TRUE)\n\ntrainSet.prep <- bake(model_rec, new_data = data_train, composition='matrix')\ntrainSet = as.data.frame((trainSet.prep))\n\ntestSet.prep<-bake(model_rec, new_data = data_test, composition='matrix')\ntestSet = as.data.frame((testSet.prep))\n```\n\n\n# Experimenting with various Models\n\nLet's first have a look at how well different models perform on the data. For the resampling method we perform a 10-fold cross validation repeated 5 times.\n\n## Initialize Models\n\n```{r}\nmyControl = trainControl(method = 'cv', \n                         number = 10, \n                         repeats = 5,\n                         verboseIter = FALSE, \n                         savePredictions = TRUE,\n                         allowParallel = T)\n\n\nparallel_start(6)\n\nset.seed(174)\nLinear.Model = train(price_m2 ~., \n                     data = trainSet, \n                     metric = 'RMSE', \n                     method = 'lm',\n                     preProcess = c('center', 'scale'),\n                     trControl = myControl)\n\n\nset.seed(174)\nGlmnet.Model = train(price_m2 ~ ., \n                     data = trainSet , \n                     metric = 'RMSE', \n                     method = 'glmnet',\n                     preProcess = c('center', 'scale'), \n                     trControl = myControl)\n\n\nset.seed(174)\nRapid.Ranger = train(price_m2 ~ ., \n                     data = trainSet, \n                     metric = 'RMSE', \n                     method = 'ranger',\n                     preProcess = c('center', 'scale'),\n                     trControl = myControl)\n\n\nset.seed(174)\nBasic.Knn <- train(price_m2 ~ .,\n             method     = \"knn\",\n             tuneGrid   = expand.grid(k =1:3), \n             trControl  = myControl, \n             metric= \"RMSE\", \n             data = trainSet)\n\n\nset.seed(174)\nXgb.Super <- train(price_m2~.,\n                   method = \"xgbTree\", \n                   tuneLength = 4,\n                   trControl = myControl,\n                   metric= \"RMSE\", \n                   data = trainSet)\n\nparallel_stop()\n```\n\n```{r}\nsuite.of.models = list(\"LINEAR.MODEL\" = Linear.Model,\n                       \"GLMNET.MODEL\" = Glmnet.Model, \n                       \"RANGER.QUEST\" = Rapid.Ranger, \n                       \"KNN.SIMPLE\" = Basic.Knn, \n                       \"XGB.SUPER\"= Xgb.Super)\n\nresamps = resamples(suite.of.models) \ndotplot(resamps, metric = 'RMSE')\n```\n\nXGBoost perfomes best on the training data. Let's test it on the test data.\n\n## TESTING MODELS ON TEST SET\n\n```{r}\nEvaluate.Prediction <- function(model, model.label, testData, ytest, grid = NULL) {\n \n  #capture prediction time\n  ptm <- proc.time()\n  # use test data to make predictions\n  pred <- predict(model, testData)\n  tm <- proc.time() - ptm\n  \n  Pred.metric<- postResample(pred = pred, obs = ytest)\n  RMSE.test <- c(Pred.metric[[1]])\n  RSquared.test <- c(Pred.metric[[2]])\n  MAE.test <- c(Pred.metric[[3]])\n  \n  \n  Summarised.results = NULL\n  if (is.null(grid)) { \n    Summarised.results = data.frame(predictor = c(model.label) ,  RMSE = RMSE.test , RSquared = RSquared.test, MAE = MAE.test, time = c(tm[[3]]))\n  } else {\n    .grid = data.frame(predictor = c(model.label) , RMSE = RMSE.test , RSquared = RSquared.test, MAE = MAE.test, time = c(tm[[3]]))\n    Summarised.results = rbind(grid, .grid)}\n  \n  \n  Summarised.results }\n\n\nMETRIC.GRID <- Evaluate.Prediction (Rapid.Ranger, \"RAPID.QUEST\", testSet, testSet$price_m2, grid=NULL)\n\nMETRIC.GRID <- Evaluate.Prediction (Glmnet.Model, \"GLMNET.MODEL\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Basic.Knn, \"KNN.SIMPLE\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Linear.Model, \"LINEAR.MODEL\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Xgb.Super, \"XGB.SUPER\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\n\nkable(METRIC.GRID[order(METRIC.GRID$RMSE, decreasing=F),]) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\nAlso here, XGBoost has the smallest RMSE and highest R2. Let's tune this model for further examination.\n\n# XGBoost Modelling\n\n## Preprocessing\n\nFirst we start again by dividing the data into training and testing samples. Again, we use `price_m2` as a strata to ensure that the distribution of the response is eqaul between the testing and training sets. Then we create a recipe.\n\n```{r}\nset.seed(123)\nsplit <- initial_split(Bilbao, prop = 0.75, \n                       strata = \"price_m2\")\ndata_train  <- training(split)\ndata_test   <- testing(split)\n\nXGB_rec <- recipe(\n  price_m2 ~ .,\n  data= data_train) %>% \n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal()) %>%\n  step_impute_knn(all_predictors(), neighbors = 10) %>% \n  prep()\n```\n\nApply pre-processing to randomly divide train data in subsets.\n\n```{r, message=FALSE}\nset.seed(123)\ncv_folds <-recipes::bake(\n    XGB_rec, \n    new_data = data_train)%>%  \n  rsample::vfold_cv(v = 5)\n\ntrain.ready<-juice(XGB_rec)\ntest.ready<-bake(XGB_rec, new_data = data_test)\n```\n\n## Modelling specifications\n\nDefine XGBoost modelling specifications and hyper parameters for tuning.\n\n```{r}\nModel.XGB <- \n  boost_tree(\n    mode = \"regression\",\n    trees = 1000,\n    min_n = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %>% \n  set_engine(\"xgboost\", objective = \"reg:squarederror\")\n```\n\n### Specify the model parameters\n\n```{r}\n# grid specification \nXGB.aspects <- \n  dials::parameters(\n    min_n(),\n    tree_depth(),\n    learn_rate(),\n    loss_reduction())\n```\n\n### Grid Space\n\nSet up a grid space which covers the hyper parameters `XGB.aspects`.\n\n```{r}\nxgboost_grid <- \ndials::grid_max_entropy(\nXGB.aspects, size = 200)\nkable(head(xgboost_grid))\n```\n\n### Create a workflow\n\n```{r}\nxgboost_wf <- \nworkflows::workflow() %>%\nadd_model(Model.XGB) %>% \nadd_formula(price_m2 ~ .)\n```\n\n### Hyper parameter searching\n\nIn this step R searches for the optimal hyper parameters by iteratively applying the different hyper parameters to multiple training samples. This step can take a while to compute.\n\n```{r, eval=FALSE}\n'parallel_start(6)\n\nTUNE.XGB <- tune::tune_grid(\n  object = xgboost_wf,\n  resamples = cv_folds,\n  grid = xgboost_grid,\n  metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::rsq_trad, yardstick::mae),\n  control = tune::control_grid(verbose = FALSE)) \n\nparallel_stop()\n\nsaveRDS(TUNE.XGB, file = here(\"Data\", \"Tune_XGB.RData\"))'\n```\n\n### Finalize optimal tune\n\nExtract parameters with lowest RMSE.\n\n```{r}\nTUNE.XGB <- readRDS(here(\"Data\", \"Houses_Bilbao\",\"Tune_XGB.RData\"))\n\nparam_final <- TUNE.XGB %>%select_best(metric = \"rmse\")\n```\n\nFinalize XGBoost with optimal tune.\n\n```{r}\nxgboost_wf2 <- xgboost_wf%>%\nfinalize_workflow(param_final)\n```\n\n### Fit the final model\n\nFit final model on the preprocessed training data.\n\n```{r}\nXGB.model <- xgboost_wf2 %>%\nfit(train.ready)\n```\n\nExtract important features.\n\n```{r}\nXGB.model %>% \n  pull_workflow_fit() %>% \n  vip()\n```\nThe number of bathrooms, whether the appartment has a lift and the construction date seems to be the most important determinants for the square meter price. \n\n### Predict on Test set.\nLet's evaluate the XGBoost model on the test data by computing the RMSE.\n\n```{r}\n# use the training model fit to predict the test data\nXGB_res <- predict(XGB.model, new_data = test.ready %>% select(-price_m2))\n\nXGB_res <- bind_cols(XGB_res, test.ready %>% select(price_m2))\n\nXGB_metrics <- metric_set(yardstick::rmse, yardstick:: mae)\n\nkable(XGB_metrics(XGB_res, truth = price_m2, estimate = .pred))\n```\n\nWe can asses the fit of the pridiction by plotting them against the actual observations from the testing samples. We can see that the model works good up to 5000 Euro per square meter. For higher the model tends to underestimate the real prices. \n```{r}\nggplot(XGB_res, aes(x = price_m2, y = .pred)) + \n    # Create a diagonal line:\n    geom_abline(lty = 2) + \n    geom_point(alpha = 0.5) + \n    labs(y = \"Predicted Sale Price\", x = \"SalePrice\") +\n    # Scale and size the x- and y-axis uniformly:\n    coord_obs_pred() +\n  theme_minimal()\n```\n\n# Explainer\nFinaly, we can explore the model prediction using the Explainer from `modelstudio()` which supplies metrics on feature importance, drop dow charts and Shapley values.\n```{r, message=FALSE, warning=FALSE}\nexplainer<- explain_tidymodels(model = XGB.model,\n                    data = test.ready,\n                    y = test.ready$price_m2,\n                    label = \"XGBoost\")\n\nmodelStudio(explainer)\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Houses_Bilbao.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"flatly","title-block-banner":true,"title":"Houses in Bilbao","subtitle":"A little Investigation","author":"Txomin Basterra Chang","date":"2023-04-15","image":"image.png","categories":["housing","Spain","code","analysis"]},"extensions":{"book":{"multiFile":true}}}}}