{"title":"Houses in Bilbao","markdown":{"yaml":{"title":"Houses in Bilbao","subtitle":"A little Investigation","author":"Txomin Basterra Chang","date":"2023-04-15","image":"image.png","categories":["housing","Spain","code","analysis"],"editor":"visual"},"headingText":"Selecting the Data","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n```\n\n```{r, message=FALSE}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\nlibrary(openxlsx)\nlibrary(lubridate)\nlibrary(here)\nlibrary(devtools)\nlibrary(recipes) \nlibrary(rsample)\nlibrary(timetk)\nlibrary(glmnet)\nlibrary(tidyquant)\nlibrary(visdat)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(caret)\nlibrary(rpart)  \nlibrary(rpart.plot)\nlibrary(pdp)\nlibrary(vip)\nlibrary(GGally)\nlibrary(car)\nlibrary(ggcorrplot)\nlibrary(ggdensity)\nlibrary(tidyquant)\nlibrary(scico)\nlibrary(paletteer)\nlibrary(earth)\nlibrary(vip)\nlibrary(ranger)\nlibrary(h2o)\nlibrary(xgboost)\nlibrary(modeltime)\nlibrary(caret)\nlibrary(lares)\nlibrary(lmtest)\nlibrary(nortest)\nlibrary(auditor)\nlibrary(DALEXtra)\nlibrary(modelStudio)\nlibrary(patchwork)\n\ntheme_set(theme_minimal())\n```\n\nBilbao is a city in northern Spain located in the province Biskaia. It is the largest metropolitan area and a economic and cultural center in the region.\n\nKnowing the housing market is valuable for sellers and buyers of real estate. Knowledge of the housing market can also provide a sence of understanding for socio-economic and socio-demographic local variations within a city.\n\nIn this analysis I will explore Bilbao's housing market in search for an understanding of the driving factors of real estate prices.\n\n\nFirst, let's have a look at the features on the presence of missing data.\n\n```{r}\nload(file = here(\"Data\", \"Houses_Bilbao\" ,\"data.Rda\"))\n\nvis_miss(data, cluster = TRUE)\n```\n\nDropping features which have no data\n\n```{r}\nBilbao <- data %>%\n  filter(loc_city == \"Bilbao\") %>%\n  select(-c(loc_city, garage, ad_description, ad_last_update, ground_size, kitchen, unfurnished, house_per_city, house_id, floor, loc_zone, loc_street, loc_neigh, obtention_date, orientation, loc_full, m2_useful)) %>%\n  mutate(price_m2 = price/m2_real) %>%\n  mutate(loc_district = gsub(\"Distrito\", \"\", loc_district)) %>%\n  mutate(bath_num = as.numeric(bath_num)) %>%\n  filter(price_m2 >= quantile(price_m2, probs = 0.001)) %>%\n  filter(price_m2 <= quantile(price_m2, probs = 0.999)) %>%\n  mutate(condition = case_when(condition == \"segunda mano/buen estado\" ~ \"used_ok\",\n                               condition == \"promoción de obra nueva\" ~ \"new\",\n                               condition == \"segunda mano/para reformar\" ~ \"used_to_reno\")) %>%\n  mutate(heating_type = case_when(heating == \"calefacción central\" ~ \"central\",\n                             heating == \"calefacción central: gas\" ~ \"central\",\n                             heating == \"calefacción central: gas propano/butano\" ~ \"central\",\n                             heating == \"calefacción central: gasoil\" ~ \"central\",\n                             heating == \"calefacción individual\" ~ \"individual\",\n                             heating == \"calefacción individual: bomba de frío/calor\" ~ \"individual\",\n                             heating == \"calefacción individual: eléctrica\" ~ \"individual\",\n                             heating == \"calefacción individual: gas\" ~ \"individual\",\n                             heating == \"calefacción individual: gas natural\" ~ \"individual\",\n                             heating == \"calefacción individual: gas propano/butano\" ~ \"individual\",\n                             heating == \"calefacción individual: gas propano/butano\" ~ \"individual\",\n                             heating == \"no dispone de calefacción\" ~ \"no_heat\"\n                             )) %>%\n  mutate(id = row_number()) %>%\n  filter(id != 3239) %>%\n  select(-c(id, m2_real, heating, price)) \n\n# Save File\nsave(Bilbao, file = here(\"Data\", \"Houses_Bilbao\", \"Bilbao.Rda\"))\n```\n\n# Summary Statistics\n\n```{r}\nBilbao %>%\n  tabyl(loc_district) %>%\n  ggplot(aes(x = \"\", y = percent, fill = loc_district)) +\n           geom_bar(stat = \"identity\", width =1, color=\"white\") +\n           coord_polar(\"y\" , start = 0) +\n  scale_fill_tq() +\n  theme_minimal() +\n  ggtitle(\"Proportion of houses per district\")\n```\n\n# Distribution of price per square meter (price_m2)\n\nRemoving outliers \\<01 and \\>99 quantiles\n\n```{r}\nBilbao %>%\n  group_by(loc_district) %>%\n  summarise(Min = min(price_m2),\n            Q25 = quantile(price_m2, probs = .25),\n            Median = median(price_m2),\n            Mean = mean(price_m2),\n            Q75 = quantile(price_m2, probs = .75),\n            Max = max(price_m2)) %>%\n  arrange(desc(Median)) %>%\n  kable(digits = 0, caption = \"Statistic of price_m2 per district\") %>%\n  kable_styling(full_width = T) \n\n\nBilbao %>%\n  summarise(Min = min(price_m2),\n            Q25 = quantile(price_m2, probs = .25),\n            Median = median(price_m2),\n            Mean = mean(price_m2),\n            Q75 = quantile(price_m2, probs = .75),\n            Max = max(price_m2)) %>%\n  arrange(desc(Median)) \n\n\nBilbao %>%\n  group_by(loc_district) %>%\n  ggplot(aes(x = loc_district, y = price_m2, fill = loc_district)) +\n  geom_boxplot() +\n  geom_hline(yintercept = 2962.963, linetype=\"dashed\", color = \"black\") + # general Median\n  scale_fill_tq() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  ggtitle(\"Boxplots of price_m2 per district\") +\n  scale_y_continuous(labels = scales::dollar_format(prefix = \"€\"))\n\n\nBilbao %>%\n  ggplot(aes(x = price_m2)) +\n  geom_histogram(bins = 100) +\n  ggtitle(\"House price/m2 in Euro\") +\n  theme_minimal() \n```\n\n# Overview of house features\n\n```{r, fig.width=11, fig.height=8}\np1 <- Bilbao %>%\n  filter(room_num <= 5) %>%\n  tabyl(room_num, loc_district) %>%\n  pivot_longer(\n    cols = c(2:13),\n    names_to = \"loc_district\",\n    values_to = \"Count\"\n  ) %>%\n  group_by(loc_district) %>%\n  mutate(Sum = sum(Count)) %>%\n  mutate(Perc = Count/Sum * 100) %>%\n  ggplot(aes(x = loc_district, y = Perc, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  facet_grid(cols = vars(room_num)) +\n  scale_fill_tq() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  labs(title = \"Percentage of Rooms per District\") +\n  theme(legend.position = \"none\")\n\np2 <- Bilbao %>%\n  ggplot(aes(x = heating_type, y = price_m2, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  scale_fill_tq() +\n  theme_minimal() +\n  labs(title = \"Price_m2 per Heating Type\") \n\np3 <- Bilbao %>%\n  tabyl(condition, loc_district) %>%\n  pivot_longer(\n    cols = c(2:13),\n    names_to = \"loc_district\",\n    values_to = \"Count\"\n  ) %>%\n  group_by(loc_district) %>%\n  ggplot(aes(x = loc_district, y = Count, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  facet_grid(cols = vars(condition)) +\n  scale_fill_tq() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) +\n  labs(title = \"Absolute Count of House Condition per District\") +\n  theme(legend.position = \"none\")\n\n\np4 <- Bilbao %>%\n  ggplot(aes(x = condition, y = price_m2, fill = loc_district)) +\n  geom_bar(position=\"dodge\", stat = \"identity\") +\n  scale_fill_tq() +\n  theme_minimal() +\n  labs(title = \"Price_m2 per Condition\") +\n  theme(legend.position = \"none\") \n\np1 + p2 + p3 + p4 + plot_layout(ncol=2, nrow = 2)\n```\n\n# Construction Date\n\n```{r}\nscatterplot(price_m2 ~ construct_date, data = Bilbao, grid = FALSE)\n```\n\n# Development of Housing and price\n\n```{r, fig.width=11, fig.height=8}\n# Hiogh density region\nBilbao %>%\n  ggplot(aes(x =construct_date, y = price_m2, fill = loc_district)) +\n  geom_hdr(probs = c (0.9, 0.5)) +\n  geom_point(shape = 21, size = 1.5) +\n  scale_fill_tq() +\n  theme_tq() +\n  labs(title = \"High density regions (Price_m2 ~ Date)\") +\n  facet_wrap(~ loc_district)\n\n\nBilbao %>%\n  filter(loc_district == \" Abando - Albia\" | loc_district == \" San Ignacio\" | loc_district == \" Otxarkoaga - Txurdinaga\") %>%\n  ggplot(aes(x =construct_date, y = price_m2, fill = loc_district)) +\n  geom_hdr(probs = c (0.9, 0.5)) +\n  geom_point(shape = 21, size = 1.5) +\n  scale_fill_tq() +\n  theme_tq() +\n  labs(title = \"High density regions certain Neighborhoods\")\n```\n\n# Looking at Correlations\n\nCorrelations between all features and response.\n\n```{r, message=FALSE, fig.width=7, fig.height=7}\n\nnumeric = Bilbao %>% \n  select(where(is.numeric)) \n\nggcorr(numeric, label = TRUE, label_size = 2, label_color = \"white\") +\n  scale_fill_paletteer_c(\"viridis::plasma\") +\n  labs(title = \"Correlations (Numeric Variables)\")\n```\n\nCorrelations with response\n\n```{r, message=FALSE, fig.width=7, fig.height=7}\nBilbao %>%\n  corr_var(price_m2)\n```\n\n# Modeling\n\n## Splitting the Data\n\nFirst we split the data into a train and test set where 3/4 of the whole data is reserved for training and the rest for testing. We also use `strata = \"price_m2\"` to make sure that the distribution of price_m2 is equal between training and test set.\n\n```{r}\nset.seed(123)\nsplit <- initial_split(Bilbao, prop = 0.75, \n                       strata = \"price_m2\")\ndata_train  <- training(split)\ndata_test   <- testing(split)\n```\n\n## Missing Data\n\nThere is missing data. Because some models (like OLS) cannot handle NAs we will interpolate these using a KNN Algorithm within out `recipe()`.\n\n```{r}\nvis_miss(Bilbao, cluster = TRUE)\n```\n\n## Create a Recipe\n\nThe function `recipe()` allows to preprocess the data before modeling. It can be applied to training and testing data. The advantage of uf using `recipe()` is that it avoids data leakage between data sets. Data leakage occurs when data transformation steps are processed on the entire data set before it is subdivided into training and testing splits. E.g., if a min-max transformation would be applied on the whole data set before splitting, the individual splits would be biased towards to global minimum and maximum. In a resampling and cross validation context, `recipe()` ensures that the data preprocessing is conducted after every iteration of data splitting.\n\n```{r}\nmodel_rec <- recipe(\n  price_m2 ~ .,\n  data= data_train) %>% \n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal()) %>%\n  step_impute_knn(all_predictors(), neighbors = 10) %>% \n  prep(training = data_train, retain=TRUE, verbose=TRUE)\n\ntrainSet.prep <- bake(model_rec, new_data = data_train, composition='matrix')\ntrainSet = as.data.frame((trainSet.prep))\n\ntestSet.prep<-bake(model_rec, new_data = data_test, composition='matrix')\ntestSet = as.data.frame((testSet.prep))\n```\n\n# EXPERIMENTING WITH VARIOUS MODELS\n\nLet's first have a look at how weel different models perform on the data. For the resampling method we perform a 10-fold cross validation repeated 5 times.\n\n```{r}\nmyControl = trainControl(method = 'cv', \n                         number = 10, \n                         repeats = 5,\n                         verboseIter = FALSE, \n                         savePredictions = TRUE,\n                         allowParallel = T)\n\n\nparallel_start(6)\n\nset.seed(174)\nLinear.Model = train(price_m2 ~., \n                     data = trainSet, \n                     metric = 'RMSE', \n                     method = 'lm',\n                     preProcess = c('center', 'scale'),\n                     trControl = myControl)\n\n\nset.seed(174)\nGlmnet.Model = train(price_m2 ~ ., \n                     data = trainSet , \n                     metric = 'RMSE', \n                     method = 'glmnet',\n                     preProcess = c('center', 'scale'), \n                     trControl = myControl)\n\n\nset.seed(174)\nRapid.Ranger = train(price_m2 ~ ., \n                     data = trainSet, \n                     metric = 'RMSE', \n                     method = 'ranger',\n                     preProcess = c('center', 'scale'),\n                     trControl = myControl)\n\n\nset.seed(174)\nBasic.Knn <- train(price_m2 ~ .,\n             method     = \"knn\",\n             tuneGrid   = expand.grid(k =1:3), \n             trControl  = myControl, \n             metric= \"RMSE\", \n             data = trainSet)\n\n\nset.seed(174)\nXgb.Super <- train(price_m2~.,\n                   method = \"xgbTree\", \n                   tuneLength = 4,\n                   trControl = myControl,\n                   metric= \"RMSE\", \n                   data = trainSet)\n\nparallel_stop()\n```\n\n```{r}\nsuite.of.models = list(\"LINEAR.MODEL\" = Linear.Model,\n                       \"GLMNET.MODEL\" = Glmnet.Model, \n                       \"RANGER.QUEST\" = Rapid.Ranger, \n                       \"KNN.SIMPLE\" = Basic.Knn, \n                       \"XGB.SUPER\"= Xgb.Super)\n\nresamps = resamples(suite.of.models) \ndotplot(resamps, metric = 'RMSE')\n```\n\nXGBoost perfomes best on the train data. Let's test it on the test data.\n\n# TESTING MODELS ON TEST SET\n\n```{r}\nEvaluate.Prediction <- function(model, model.label, testData, ytest, grid = NULL) {\n \n  #capture prediction time\n  ptm <- proc.time()\n  # use test data to make predictions\n  pred <- predict(model, testData)\n  tm <- proc.time() - ptm\n  \n  Pred.metric<- postResample(pred = pred, obs = ytest)\n  RMSE.test <- c(Pred.metric[[1]])\n  RSquared.test <- c(Pred.metric[[2]])\n  MAE.test <- c(Pred.metric[[3]])\n  \n  \n  Summarised.results = NULL\n  if (is.null(grid)) { \n    Summarised.results = data.frame(predictor = c(model.label) ,  RMSE = RMSE.test , RSquared = RSquared.test, MAE = MAE.test, time = c(tm[[3]]))\n  } else {\n    .grid = data.frame(predictor = c(model.label) , RMSE = RMSE.test , RSquared = RSquared.test, MAE = MAE.test, time = c(tm[[3]]))\n    Summarised.results = rbind(grid, .grid)}\n  \n  \n  Summarised.results }\n\n\nMETRIC.GRID <- Evaluate.Prediction (Rapid.Ranger, \"RAPID.QUEST\", testSet, testSet$price_m2, grid=NULL)\n\nMETRIC.GRID <- Evaluate.Prediction (Glmnet.Model, \"GLMNET.MODEL\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Basic.Knn, \"KNN.SIMPLE\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Linear.Model, \"LINEAR.MODEL\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\nMETRIC.GRID <- Evaluate.Prediction (Xgb.Super, \"XGB.SUPER\", testSet, testSet$price_m2, grid=METRIC.GRID)\n\n\nkable(METRIC.GRID[order(METRIC.GRID$RMSE, decreasing=F),]) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\nAlso here, XGBoost has the smallest RMSE and highest R2. Let's tune this model for further examination\n\n# XGBoost Modelling\n\n## Preprocessing\n\nFirst we start again by divide data into training and testing samples. Again, we use `price_m2` as a strata to ensure that the distribution of the response is eqaul between the testing and training sets. Then we create a recipe.\n\n```{r}\nset.seed(123)\nsplit <- initial_split(Bilbao, prop = 0.75, \n                       strata = \"price_m2\")\ndata_train  <- training(split)\ndata_test   <- testing(split)\n\nXGB_rec <- recipe(\n  price_m2 ~ .,\n  data= data_train) %>% \n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal()) %>%\n  step_impute_knn(all_predictors(), neighbors = 10) %>% \n  prep()\n```\n\nApply pre-processing to randomly divide train data in subsets.\n\n```{r, message=FALSE}\nset.seed(123)\ncv_folds <-recipes::bake(\n    XGB_rec, \n    new_data = data_train)%>%  \n  rsample::vfold_cv(v = 5)\n```\n\nTransform train and test data with recipe\n\n```{r, message = FALSE, warning = FALSE, results = FALSE, include = FALSE}\ntrain.ready<-juice(XGB_rec)\ntest.ready<-bake(XGB_rec, new_data = data_test)\n```\n\n## Modelling specifications\n\nDefine XGBoost modelling specifications and to be tuned hyperparameters\n\n```{r}\nModel.XGB <- \n  boost_tree(\n    mode = \"regression\",\n    trees = 1000,\n    min_n = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %>% \n  set_engine(\"xgboost\", objective = \"reg:squarederror\")\n```\n\n### Specify the model parameters\n\n```{r}\n# grid specification \nXGB.aspects <- \n  dials::parameters(\n    min_n(),\n    tree_depth(),\n    learn_rate(),\n    loss_reduction())\n```\n\n### Srid Space\n\nSet up a grid space which covers the hyperparameters `XGB.aspects`.\n\n```{r}\nxgboost_grid <- \ndials::grid_max_entropy(\nXGB.aspects, size = 200)\nkable(head(xgboost_grid))\n```\n\n### Create a workflow\n\n```{r}\nxgboost_wf <- \nworkflows::workflow() %>%\nadd_model(Model.XGB) %>% \nadd_formula(price_m2 ~ .)\n```\n\n### Hyperparameter searching\n\nIn this step R searches for the optimal hyperparameters by iteratively applying the different hyperparameters to multiple trainig samples. This step can take a while to compute.\n\n```{r, eval=FALSE}\n'parallel_start(6)\n\nTUNE.XGB <- tune::tune_grid(\n  object = xgboost_wf,\n  resamples = cv_folds,\n  grid = xgboost_grid,\n  metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::rsq_trad, yardstick::mae),\n  control = tune::control_grid(verbose = FALSE)) \n\nparallel_stop()\n\nsaveRDS(TUNE.XGB, file = here(\"Data\", \"Tune_XGB.RData\"))'\n```\n\n### Finalize optimal tune\n\nExtract parameters with lowest rmse\n\n```{r}\nTUNE.XGB <- readRDS(here(\"Data\", \"Houses_Bilbao\",\"Tune_XGB.RData\"))\n\nparam_final <- TUNE.XGB %>%select_best(metric = \"rmse\")\n```\n\nFinalize XGBoost with optimal tune.\n\n```{r}\nxgboost_wf2 <- xgboost_wf%>%\nfinalize_workflow(param_final)\n```\n\n### Fit the final model\n\n#### Fit final model on the preprocessed training data.\n\n```{r}\nXGB.model <- xgboost_wf2 %>%\nfit(train.ready)\n```\n\nExtract important features.\n\n```{r}\nXGB.model %>% \n  pull_workflow_fit() %>% \n  vip()\n```\n\n#### Predict on thest set.\n\n```{r}\n# use the training model fit to predict the test data\nXGB_res <- predict(XGB.model, new_data = test.ready %>% select(-price_m2))\n\nXGB_res <- bind_cols(XGB_res, test.ready %>% select(price_m2))\n\nXGB_metrics <- metric_set(yardstick::rmse, yardstick:: mae)\n\nkable(XGB_metrics(XGB_res, truth = price_m2, estimate = .pred))\n\n\nggplot(XGB_res, aes(x = price_m2, y = .pred)) + \n    # Create a diagonal line:\n    geom_abline(lty = 2) + \n    geom_point(alpha = 0.5) + \n    labs(y = \"Predicted Sale Price\", x = \"SalePrice\") +\n    # Scale and size the x- and y-axis uniformly:\n    coord_obs_pred() +\n  theme_minimal()\n```\n\n# Explainer\n\n```{r, message=FALSE, warning=FALSE}\nexplainer<- explain_tidymodels(model = XGB.model,\n                    data = test.ready,\n                    y = test.ready$price_m2,\n                    label = \"XGBoost\")\n\nmodelStudio(explainer)\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"Houses_Bilbao.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"flatly","title-block-banner":true,"title":"Houses in Bilbao","subtitle":"A little Investigation","author":"Txomin Basterra Chang","date":"2023-04-15","image":"image.png","categories":["housing","Spain","code","analysis"]},"extensions":{"book":{"multiFile":true}}}}}